"""
Generated LLM Integration for {{ llm_name }}
Provider: {{ provider }} | Model: {{ model_name }}
"""
from __future__ import annotations

import os
from pydantic import BaseModel

# Provider interface and request/response types
from llm.providers.base import LLMRequest

# Provider implementation import (generated)
{% if provider == "gemini" %}
from llm.providers.gemini import GeminiProvider as ProviderImpl
API_KEY_ENV = "GEMINI_API_KEY"
{% elif provider == "openai" %}
from llm.providers.openai import OpenAIProvider as ProviderImpl
API_KEY_ENV = "OPENAI_API_KEY"
{% elif provider == "anthropic" %}
from llm.providers.anthropic import AnthropicProvider as ProviderImpl
API_KEY_ENV = "ANTHROPIC_API_KEY"
{% else %}
raise RuntimeError("Unsupported provider: {{ provider }}")
{% endif %}


class {{ llm_name }}Input(BaseModel):
    prompt: str


class {{ llm_name }}Output(BaseModel):
    response: str


class {{ llm_name }}LLM:
    def __init__(self) -> None:
        api_key = os.getenv(API_KEY_ENV, "")
        if not api_key:
            raise ValueError(
                f"Missing API key for provider '{{ provider }}'. Set environment variable {API_KEY_ENV}."
            )
        self.provider = ProviderImpl(api_key=api_key, model="{{ model_name }}")
        self.model = "{{ model_name }}"
        self.max_tokens = {{ max_tokens }}
        self.temperature = {{ temperature }}

    async def generate(self, prompt: str) -> str:
        request = LLMRequest(
            messages=[{"role": "user", "content": prompt}],
            model=self.model,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
        )
        response = await self.provider.chat(request)
        return response.content
